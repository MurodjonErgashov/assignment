{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2578fb8b-d207-43d2-91da-41551717e37c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Quality Check"
    }
   },
   "outputs": [],
   "source": [
    "def check_duplicates(df, id):\n",
    "    groupped_df = df.groupBy(id).count()\n",
    "    ids = groupped_df.filter(groupped_df['count'] > 1).select(id).collect()\n",
    "    return df.filter(df[id].isin(ids)) \\\n",
    "        .withColumn('DataQuality', lit('Duplicate in Key Column'))\n",
    "                    \n",
    "def check_to_nulls(df, column):\n",
    "    filtered_df = df.filter(df[column].isNull())\n",
    "    return filtered_df.withColumn('DataQuality', lit('Null in Column'))\n",
    "\n",
    "def check_for_valid_state(df, column, states):\n",
    "    filtered_df = df.filter(~df[column].isin(states))\n",
    "    return filtered_df.withColumn('DataQuality', concat(lit('Invalid State of'), df[column]))\n",
    "\n",
    "def check_for_email_format(df, colm):\n",
    "    email_regex = r\"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$\"\n",
    "\n",
    "    df_validated = df.filter(~df[colm].rlike(email_regex))\n",
    "    df_validated = df_validated.withColumn('DataQuality', concat(lit('Invalid Email '), df_validated[colm]))\n",
    "    return df_validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b70b067-fa66-4ebf-88a6-1c6fe09313bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Consistency check"
    }
   },
   "outputs": [],
   "source": [
    "def check_logical_date(df, start_date, end_date):\n",
    "    return df.filter(to_date(df[start_date]) > to_date(df[end_date])) \\\n",
    "        .withColumn('DataQuality', concat(lit('Invalid Date of '), df[start_date], lit(' and '), df[end_date]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4541abfe-1c87-4159-8780-2c65df7c5687",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dectypt/Encrypt"
    }
   },
   "outputs": [],
   "source": [
    "from cryptography.fernet import Fernet\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "\n",
    "secret_key = 'Tg-Bs_QsJhsFCFMb9uK1J2jOhV7VEUyh2-UtrchG5Ow='\n",
    "# dbutils.secrets.get(scope = \"key-vault-secret\", key = \"fernet_key_name\")\n",
    "# secret_key = Fernet.generate_key()\n",
    "cipher = Fernet(secret_key)\n",
    "\n",
    "def encrypt_value(val):\n",
    "    if val:\n",
    "        val = cipher.encrypt(val.encode()).decode()\n",
    "    return udf(val, StringType())\n",
    "\n",
    "def decrypt_value(val):\n",
    "    if val:\n",
    "        val = cipher.decrypt(val.encode()).decode()\n",
    "    return udf(val, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "def6835c-d92a-468b-bb4f-83e47496b114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_data_lake(baseDF: DataFrame, wrtFormat: StringType, \n",
    "                        wrtMode: StringType, wrtPath: StringType, wrtKey: StringType):\n",
    "    if wrtMode == \"overwrite\":\n",
    "        \n",
    "        baseDF.write.format(wrtFormat) \\\n",
    "            .mode(\"overwrite\")\\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .save(wrtPath)\n",
    "\n",
    "    elif wrtMode == \"merge\":\n",
    "        try:\n",
    "            oldDataTable = DeltaTable.forPath(spark, wrtPath)\n",
    "\n",
    "            oldDataTable.alias(\"dsOldData\").merge(\\\n",
    "            baseDF.alias(\"dsNewData\"),f\"dsOldData.{wrtKey} = dsNewData.{wrtKey}\" )\\\n",
    "            .whenMatchedUpdateAll()\\\n",
    "            .whenNotMatchedInsertAll()\\\n",
    "            .execute()\n",
    "        except Exception as e:\n",
    "            print(\"Exception:\" , e)\n",
    "    if wrtMode == \"Append\":\n",
    "        baseDF.write.format(wrtFormat) \\\n",
    "            .mode(\"append\")\\\n",
    "            .save(wrtPath)\n",
    "            # .option(\"mergeSchema\", True)\\\n",
    "    if wrtMode == 'scd2':\n",
    "        baseDF_scd2 = baseDF.withColumn(\"is_active\", lit(True)) \\\n",
    "                     .withColumn(\"start_date_eff\", current_timestamp()) \\\n",
    "                     .withColumn(\"end_date_eff\", lit(None).cast(\"timestamp\")) \n",
    "        \n",
    "        oldDataTable = DeltaTable.forPath(spark, wrtPath)\n",
    "        merge_condition = f\"dsOldData.EmpID = dsNewData.EmpID AND dsOldData.is_active = True\"\n",
    "\n",
    "        oldDataTable.alias(\"dsOldData\").merge(\n",
    "            baseDF_scd2.alias(\"dsNewData\"),\n",
    "            merge_condition\n",
    "        ).whenMatchedUpdate(\n",
    "            condition = \" OR \" + \" OR \".join([f\"dsOldData.{col} <> dsNewData.{col}\" for col in baseDF.columns if col != 'EmpID']),\n",
    "            set = {\n",
    "            \"is_active\": lit(False),\n",
    "            \"end_date_eff\": current_timestamp()\n",
    "        }\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5e98ea5-82c2-4acf-ac87-7727ff736abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_dw_columns(baseDF : DataFrame , lsKeyColumns: List, runId: str, dataSourceName: str) -> DataFrame:\n",
    "  baseDF = (baseDF\n",
    "            .withColumn(\"W_DATA_SOURCE\",f.lit(dataSourceName))\n",
    "            .withColumn(\"W_INSERT_DT\",f.current_timestamp())\n",
    "            .withColumn(\"W_UPDATE_DT\",f.current_timestamp())\n",
    "            .withColum(\"Run_Id\", f.lit(runId))\n",
    "           )\n",
    "  \n",
    "  baseDF = baseDF.withColumn(\"INTEGRATION_ID\",f.concat_ws('-', *lsKeyColumns))\n",
    "\n",
    "  return baseDF\n",
    "\n",
    "def convert_date_columns(df):\n",
    "    #only for columns that end with _DATE\n",
    "    df = df.withColumn(\"date_len\",f.lit(10))\n",
    "    col_list = df.columns\n",
    "    date_cols = []\n",
    "    for col in col_list:\n",
    "        if(col[-5:]=='_DATE'):\n",
    "            if(str(df.schema[col].dataType)=='StringType()'):\n",
    "                date_cols.append(col)\n",
    "                df = df.withColumn('temp_date',f.left(df[col],df['date_len']))\n",
    "                df = df.withColumn(col,to_date(df['temp_date'],'yyyy-MM-dd')).drop('temp_date')\n",
    "    df = df.drop(\"date_len\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "606461c5-a804-4a72-bddd-5d368bf87e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "UTIL_FUNCTIONS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}